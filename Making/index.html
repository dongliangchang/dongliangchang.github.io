<!DOCTYPE html>
<html>

<head>
<!-- Google tag (gtag.js) -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-47WTV4E2V1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-47WTV4E2V1');
</script>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    <title>Making a Bird AI Expert Work for You and Me</title>
    <link rel="stylesheet" href="w3.css">
</head>

<body>

    <br />
    <br />

    <div class="w3-container">
        <div class="w3-content" style="max-width:1080px">
            <div class="w3-content w3-center" style="max-width:850px">
                <h2 id="title"><b>Making a Bird AI Expert Work for You and Me</b></h2>
                <p>
                    <a href="https://www.dongliangchang.cn//" target="_blank">Dongliang Chang</a><sup>1</sup>
                    &nbsp;&nbsp;&nbsp;&nbsp;
                    <a href="https://kyp-sketchx.github.io/" target="_blank">Kaiyue Pang</a><sup>2</sup>
                    &nbsp;&nbsp;&nbsp;&nbsp;
                    <a href="https://scholar.google.com/citations?user=DbRZSaoAAAAJ&hl=en" target="_blank">Ruoyi Du</a><sup>2</sup>
                    &nbsp;&nbsp;&nbsp;&nbsp;
                    <a href="http://www.pris.net.cn/introduction/teacher/zhanyu_ma" target="_blank">Zhanyu Ma</a><sup>1*</sup>
                    &nbsp;&nbsp;&nbsp;&nbsp;
                    <a href="http://personal.ee.surrey.ac.uk/Personal/Y.Song/" target="_blank">Yi-Zhe Song</a><sup>2</sup>
                    &nbsp;&nbsp;&nbsp;&nbsp;
                    <a href="http://www.pris.net.cn/introduction/teacher/guojun" target="_blank">Jun Guo</a><sup>1</sup>
                </p>
                <p>
                    <sup>1</sup>Beijing University of Posts and Telecommunications, CN
                    &nbsp; &nbsp; &nbsp;
                    <sup>2</sup>SketchX, CVSSP, University of Surrey, UK
                </p>
                <!-- <p><b>CVPR 2021</b></p> -->
                <div class="w3-content w3-center" style="max-width:850px">
                    <div style="max-width:850px; display:inline-block">
                    <a href="https://arxiv.org/pdf/2112.02747.pdf" target="_blank" style="color:#007bff">
                            <img src="Making.jpg" alt="front" style="width:50px"/>
                            <div style="margin:10px 0"></div>
                            <b>arXiv</b></a>
<!--                     </div>
                    &emsp;&emsp;&emsp;&emsp;&emsp;
                    <div style="max-width:850px; display:inline-block">
                    <a href="https://drive.google.com/file/d/1Lh06RY0UxJDW5t3YWZsLqxyK-tLX3iKX/view?usp=sharing" target="_blank" style="color:#007bff">
                            <img src="database.svg" alt="front" style="width:50px"/>
                            <div style="margin:10px 0"></div>
                            <b>Dataset</b></a> -->
                    </div>
                    &emsp;&emsp;&emsp;&emsp;&emsp;
                    <div style="max-width:850px; display:inline-block">
                    <a href="https://github.com/PRIS-CV/Making-a-Bird-AI-Expert-Work-for-You-and-Me" target="_blank" style="color:#007bff">
                            <img src="github.png" alt="front" style="width:65px"/>
                            <div style="margin:10px 0"></div>
                            <b>Code</b></a>
                    </div>
                </div>
            </div>

            <br>
            <div class="w3-content w3-center" style="max-width:850px">
                <img src="figure1.jpg" alt="front" style="width:450px"/>
                <p>Figure 1. AI Bird Expert Enriches Human Bird Knowledge. By retreating from the common goal of a FGVC model in pursuing better expert label predictions, we envision a human-centred FGVC endeavour with a three-way cycle for human knowledge consumption and propose a first solution.</p>
            </div>
            <br>
            <h3 class="w3-left-align" id="introduction"><b>Introduction</b></h3>
            <p>
As powerful as fine-grained visual classification (FGVC) is, responding your query with a bird name of ``Whip-poor-will" or ``Mallard" probably does not make much sense. This however commonly accepted in the literature, underlines a fundamental question interfacing AI and human -- what constitutes transferable knowledge for human to learn from AI? This paper sets out to answer this very question using FGVC as a test bed. Specifically, we envisage a scenario where a trained FGVC model (the AI expert) functions as a knowledge provider in enabling average people (you and me) to become better domain experts ourselves, i.e., those capable in distinguishing between ``Whip-poor-will" and ``Mallard".
Fig.1 lays out our approach in answering this question. Assuming an AI expert trained using expert human labels, we ask (i) what is the best transferable knowledge we can extract from AI, and (ii) what is the most practical means to measure the gains in expertise given that knowledge? On the former, we propose to represent knowledge as highly discriminative visual regions that are expert-exclusive. For that, we devise a multi-stage learning framework, which starts with modelling visual attention of domain experts and novices separately, before discriminatively distilling their differences to acquire those exclusive to experts. For the latter, we simulate the evaluation process as a book guide to best accommodate the learning practice of that is accustomed to humans. A comprehensive human study of 15,000 trials shows our method is able to consistently improve people of divergent bird expertise to recognise once unrecognisable birds. To counter the notorious lack of reproducibility of perceptual studies, and in turn to make a sustainable direction out of our ``AI for Human'' effort, we further propose a quantitative metric, namely Transferable Effective Model Attention (TEMI). TEMI acts as a crude but benchmarkable metric to replace large-scale human studies, and therefore allows future efforts in this direction to be comparable to ours. We attest the integrity of TEMI by (i) empirically showing a strong correlation between TEMI scores and raw human study data, and (ii) its expected behaviour holds for a large body of attention models. Last but not least, our approach also leads to improved FGVC performance in the conventional benchmarking sense, when the extracted knowledge defined is utilised as means to achieve discriminative localisation.
            </p>
<!--             <div class="w3-content w3-center" style="max-width:1000px">
            <video style="width:840px; height:473px" controls="controls">
                <source src="Flamingo-CVPR21.mp4" type="video/mp4" />
            </video>
            </div> -->
            <h3 class="w3-left-align"><b>Our Solution</b></h3>
            <!-- <h4 class="w3-left-align"><b>Dataset Preview</b></h4> -->
            <div class="w3-content w3-center" style="max-width:1000px">
                <img src="figure2.jpg" alt="dataset" style="width:800px" />
                <p class="w3-left-align">
Figure 2. Schematic illustration of how to obtain expert-exclusive but highly discriminative visual regions S_δ via our approach (Sec. 2).
(a) We assume the functioning of a visual classification model as an attention sampling process from a pre-defined pool
of visual regions F_pool (global and local image patches). (b) We define the regions most effective for human knowledge
consumption as a subset of visual attentions of domain experts only S_δ. We model regions attended by domain novices
S_novice with the best possible fine-grained amateur textural descriptions of an image respectively – see Sec. 3.4. (c) Our
final refined elections of S_δ from F_pool are that of the most discriminative.
                </p>
            </div>

            <h3 class="w3-left-align"><b>Human Study</b></h3>
            <!-- <h4 class="w3-left-align"><b>Dataset Preview</b></h4> -->
            <div class="w3-content w3-center" style="max-width:1000px">
                <img src="figure3.jpg" alt="dataset" style="width:800px" />
                <p class="w3-left-align">
Figure 3. Sample questionnaire for measuring the efficacy of AI-empowered knowledge by simulating it as a book guide. In
data and participant setup stage, S_δ is not shown.
</p>
            </div>

            <div class="w3-content w3-center" style="max-width:1000px">
                <img src="figure4.jpg" alt="dataset" style="width:800px" />
                <p class="w3-left-align">
Figure 4. Qualitative and quantitative evidence of $S_{\delta}$/$\hat{S}_{\delta}$.} Top row: box plot to demonstrate the efficacy of $S_{\delta}$/$\hat{S}_{\delta}$ in helping people reverse their failed decision for bird recognition. Green triangle represents the mean performance. Bottom two rows: sample illustration of top K visual regions $S_{\delta}$/$\hat{S}_{\delta}$ attend to.
</p>
            </div>
<!--             <h4 class="w3-left-align"><b>Explore More PQA Pairs</b></h4>
            <div class="w3-container">
                <div class="w3-half">
                    <div class="w3-container">
                        <p>
                            We provide visualization of more PQA pairs below. Simply select a task and an index to view.
                        </p>
                        <div class="w3-content w3-center w3-half">
                        <b>Question</b>
                        </div>
                        <div class="w3-content w3-center w3-half">
                        <b>Answer</b>
                        </div>
                        <p class="w3-center">
                        <img id="showQuestion" src="PQA_pair/Closure Filling/1_Q.svg" style="width:230px; height:230px" alt="front" />
                            &emsp;
                        <img id="showAnswer" src="PQA_pair/Closure Filling/1_A.svg" style="width:230px; height:230px" alt="front" />
                        <script>
                            var t; var i;
                            function change(v) {
                                changeTask(); changeImg(v);
                                var task = t; var image = i;
                                document.getElementById("showQuestion").src = "PQA_pair/" + task.value + "/" + image + "_Q.svg";
                                document.getElementById("showAnswer").src = "PQA_pair/" + task.value + "/" + image + "_A.svg";
                            }
                            function changeTask() {
                                t = document.getElementById("task-selector");
                            }
                            function changeImg(v) {
                                if(v>=1 && v<=10)
                                    i = v;
                            }
                        </script>
                        </p>
                    </div>
                </div> -->

<!--                 <div class="w3-half">
                    <div class="w3-container">
                        <h4>Raw Data Format</h4> -->
<!--                            There are 7 folder in the dataset root folder each represent one of 7 tasks. Each folder contain 20k-->
<!--                            json files.-->
<!--                            All file saved with <a href="https://github.com/fchollet/ARC" target="_blank">ARC</a> data format and can be visualized-->
<!--                            by <a href="https://github.com/fchollet/ARC/tree/master/apps" target="_blank">ARC visualization tool</a>.-->
<!--                         <div class="w3-code">
                            All PQA pairs are stored in JSON file. Each JSON file contains a dictionary with two fields:

                            <br>
                            - "train": a list of exemplar Q/A pairs.

                            <br>
                            - "test": a list of test Q/A pairs.
                            <br>
                            <br>
                            Each "pair" has two fields:
                            <br>
                            - "input": a question "grid".
                            <br>
                            - "output": an output "grid".
                            <br>
                            <br>
                            Each "grid" (width w, height h) is composed of w*h color symbols. Each color symbol is one of 10 pre-defined colors.
                        </div>
                    </div>
                </div>
            </div> -->
<!--             <div class="w3-container">
                <div class="w3-half">
                    <form class="w3-container w3-center" action="" style="display:inline-block">
                        <select id="task-selector" onchange="change(this.value)" name="Task" style="width:170px; height:36px" >
                            <option selected value="Closure Filling">(a)Closure Filling</option>
                            <option value="Continuity Connection">(b)Continuity Connection</option>
                            <option value="Proximity Identification">(c)Proximity Identification</option>
                            <option value="Shape Reconstruction">(d)Shape Reconstruction</option>
                            <option value="Shape&Pattern Similarity">(e)Shape&Pattern Similarity</option>
                            <option value="Reflection Symmetry">(f)Reflection Symmetry</option>
                            <option value="Rotation Symmetry">(g)Rotation Symmetry</option>
                        </select>
                    </form>

                    <div class="w3-bar w3-round" id="image-selector" style="height:36px; display:inline-block">
                        <button class="w3-bar-item w3-hover-blue" value="1" onclick="change(this.value)">1</button >
                        <button class="w3-bar-item w3-hover-blue" value="2" onclick="change(this.value)">2</button >
                        <button class="w3-bar-item w3-hover-blue" value="3" onclick="change(this.value)">3</button >
                        <button class="w3-bar-item w3-hover-blue" value="4" onclick="change(this.value)">4</button >
                        <button class="w3-bar-item w3-hover-blue" value="5" onclick="change(this.value)">5</button >
                        <button class="w3-bar-item w3-hover-blue" value="6" onclick="change(this.value)">6</button >
                        <button class="w3-bar-item w3-hover-blue" value="7" onclick="change(this.value)">7</button >
                        <button class="w3-bar-item w3-hover-blue" value="8" onclick="change(this.value)">8</button >
                        <button class="w3-bar-item w3-hover-blue" value="9" onclick="change(this.value)">9</button >
                        <button class="w3-bar-item w3-hover-blue" value="10" onclick="change(this.value)">10</button >
                    </div>
                </div>

                <div class="w3-half">
                    <div class="w3-container">
                        <a href="template.json" target="_blank">
                            <button class="w3-btn w3-white w3-border w3-border-blue w3-hover-blue w3-round-large">An Example of JSON file</button></a>
                    </div>
                </div>
            </div> -->


<!--            <h4 class="w3-left-align"><b>Statistic</b></h4>-->
<!--            <div class="w3-cell-row">-->

<!--                <div class="w3-container w3-cell w3-cell-middle">-->
<!--                    <div class="w3-content w3-center">-->
<!--                        <img src="statistic.svg" alt="statistic"  style="width:80%"/>-->
<!--                        <p>-->
<!--                            Figure 3.-->
<!--                            (a) Distribution of key region loca-tions. -->
<!--                            (b) Distribution of grid size.-->
<!--                        </p>-->
<!--                    </div>-->
<!--                </div>-->
<!--              -->
<!--                <div class="w3-container w3-cell w3-cell-middle">-->
<!--                    <div class="w3-content w3-center" style="width:120%" >-->
<!--                        <table class="w3-table w3-bordered w3-border">-->
<!--                            <tr>-->
<!--                                <th>Tasks </th>-->
<!--                                <th>T<sup>1</sup></th>-->
<!--                                <th>T<sup>2</sup></th>-->
<!--                                <th>T<sup>3</sup></th>-->
<!--                                <th>T<sup>4</sup></th>-->
<!--                                <th>T<sup>5</sup></th>-->
<!--                                <th>T<sup>6</sup></th>-->
<!--                                <th>T<sup>7</sup></th>-->
<!--                            </tr>-->
<!--                            <tr>-->
<!--                                <td>Avg Symbols</td>-->
<!--                                <td>2.0</td>-->
<!--                                <td>2.0</td>-->
<!--                                <td>5.0</td>-->
<!--                                <td>2.0</td>-->
<!--                                <td>5.0</td>-->
<!--                                <td>3.0</td>-->
<!--                                <td>5.0</td>-->
<!--                            </tr>-->
<!--                            <tr>-->
<!--                                <td>Avg Slots (%)</td>-->
<!--                                <td>12.9</td>-->
<!--                                <td>3.6</td>-->
<!--                                <td>4.0</td>-->
<!--                                <td>7.6</td>-->
<!--                                <td>15.3</td>-->
<!--                                <td>9.8</td>-->
<!--                                <td>12.5</td>-->
<!--                            </tr>-->
<!--                        </table>-->
<!--                        <p> Table 1. Statistics of PQA dataset. </p>-->
<!--                    </div>-->
<!--                </div>-->
<!--              -->
<!--            </div>-->

<!--            <p>-->
<!--                Statistical analysis is provided as shown in Figure 3 and Table 1 above where Avg Symbols indicate the-->
<!--                number of symbols in a question-grid.-->
<!--                The x and y coordinates in Figure 3 (a) are normalized to (0,1), corresponding to the center of key regions, -->
<!--                x-axis and y-axis in Figure 3 (b) correspond to width and height of a grid.-->
<!--                It basically shows the number of colors that are enough to represent a specified instance of a task.-->
<!--                For instance, 2 colors in T1 are enough &#45;&#45; one for background and one for the boundary of closure-->
<!--                region.-->
<!--                Avg Slots represents the percentage of question-grid needed to be modified to form a correct answer.-->
<!--            </p>-->

            <h3 class="w3-left-align"><b> TEMI: Human-Like Evaluation without Humans</b></h3>
            <p>
To counter the notorious lack of reproducibility of perceptual studies, and in turn to make a sustainable direction out of our “AI for Human” effort, we further propose a quantitative metric, namely Transferable Effective Model Attention (TEMI). TEMI acts as a crude but benchmarkable metric to replace large-scale human studies,
and therefore allows future efforts in this direction to be comparable to ours. We attest the integrity of TEMI by (i) empirically showing a
strong correlation between TEMI scores and raw human study data, and (ii) its expected behaviour holds for a large body of attention
models.
            </p>
            <div class="w3-content w3-center" style="max-width:1000px">
                <img src="figure5.jpg" alt="network" style="width:800px" />
                <p>
                    Figure 5.  Comparisons of TEMI scores for the visual attention maps obtained by our proposed method and those presented in the 16 existing relevant works.} We demonstrate all stats needed to calculate Improvability and Specificity (Eq.~\ref{eq:comp-1} \& \ref{eq:comp-2}), both of which are then used to represent TEMI (Eq.~\ref{eq:comp-3}). Towards fair evaluation, we re-implement the 16 exiting methods from their public released codes on our pre-processed CUB bird data with same training strategy. For methods that pursue optimal classification performance without a specified attention visualisation mechanism, we resort to GradCAM \cite{selvaraju2017grad} and TransRelevance \cite{chefer2021transformer} as two generic ways for CNN and Transformer type of models, which covered most network backbone choices across disciplines in computer vision world nowadays.
<!--                    The encoder takes test question embedding, positional encoding and context embedding as inputs,-->
<!--                     where context embedding is given by a context encoder, providing clues about the implied law in an example PQA pair, and the positional encoding adapts to the 2D case. -->
<!--                    The decoder can generate an answer-grid by predicting all symbols at every location in parallel.-->
                </p>
            </div>


            <h3 class="w3-left-align"><b>Visualization</b></h3>
            <div class="w3-content w3-center" style="max-width:1000px">
                <img src="figure6.jpg" alt="network" style="width:800px" />
                <p>
                    Figure 6. Qualitative comparisons of the recognition decisive regions between our proposed $S_{\delta}$ and other methods.}} For NTS, DCL, CAL, we leverage GradCAM \cite{selvaraju2017grad} to detect model attentions in the input image space with a unified approach. For CVE, INTER, SmoothGrad, PathwayGrad, we re-implement their public released code on our pre-processed CUB bird data.

<!--                    The encoder takes test question embedding, positional encoding and context embedding as inputs,-->
<!--                     where context embedding is given by a context encoder, providing clues about the implied law in an example PQA pair, and the positional encoding adapts to the 2D case. -->
<!--                    The decoder can generate an answer-grid by predicting all symbols at every location in parallel.-->
                </p>
            </div>

<!-- 
            <p>To further evaluate the training efficiency of each model, we provide different amounts of data for training.
                We can observe from Figure 4 that the scale of training data significantly affects model's performance.
                Unlike our model, humans can learn the task-specific rule from very limited examples.
                This clearly signifies just how unexplored this topic is, and in turn encourages future research to progress towards human-level intelligence.
            </p>

            <div class="w3-content w3-center" style="max-width:850px">
                <img src="result.svg" alt="result" style="width:600px" />
                <p> Figure 4. Testing results on varying training data volume. </p>
            </div> -->



<!--            <p>-->
<!--                We observe our method which significantly outperforms other competitors over all tasks.-->
<!--                On inspecting performances of every task further individually in Table2, we realize that T5 is most-->
<!--                challenging as all baseline methods fail completely.-->
<!--                On the contrary, it is interesting to note that humans can understand and address the questions in T5-->
<!--                quite easily.-->
<!--                Similar trend can be found on T6 and T7 as well. On tasks T1 to T4, all competitors perform better than-->
<!--                they do on T5 to T7.-->
<!--                Furthermore TD+H-CNN achieves result comparable to ours on T4.-->
<!--                To further evaluate the training efficiency of each model, we provide different amounts of data for-->
<!--                training.-->
<!--                We can observe from Figure5 that the scale of training data significantly affects model's performance.-->
<!--                Unlike our model, humans can learn the task-specific rule from very limited examples.-->
<!--                Basically all methods would nearly fail if we reduce the amount of training data to 15% of PQA pairs per-->
<!--                task.-->
<!--                Compared to other baseline methods however, ours performs the best.-->
<!--            </p>-->

<!--            <h3 class="w3-left-align" id="publication"><b>Publication</b></h3>-->

<!--            <h4 class="w3-left-align" id="github"><b>Code</b></h4>-->
<!--            <a href="https://github.com/qugank/PQA/code" target="__blank">GitHub</a>-->
<!--            |-->
<!--            <a href="https://drive.google.com/file/d/1FW2SMdd68U2KpSxTG4QEYQfexVn1KjK2/view?usp=sharing"-->
<!--                target="__blank">trained weight</a>-->

<!--            <h4 class="w3-left-align" id="dataset"><b>Dataset</b></h4>-->
<!--            <a href="https://drive.google.com/file/d/1GOIiqUuRy1SCXMDoDRPB_sxRuTmd8XYS/view?usp=sharing"-->
<!--                target="__blank">download</a>-->

<!--            <p>-->
<!--                There are 7 folder in the dataset root folder each represent one of 7 tasks. Each folder contain 20k-->
<!--                json files.-->
<!--                All file saved with <a href="https://github.com/fchollet/ARC">ARC</a> data format and can be visualized-->
<!--                by <a href="https://github.com/fchollet/ARC/tree/master/apps">ARC visualization tool</a>.-->
<!--            </p>-->

<!--            <div class="w3-code">-->
<!--                Each JSON file contains a dictionary with two fields:-->
<!--                <br>-->
<!--                "train": demonstration input/output pairs. It is a list of "pairs" (typically 3 pairs).-->
<!--                <br>-->
<!--                "test": test input/output pairs. It is a list of "pairs" (typically 1 pair).-->
<!--                <br>-->
<!--                <br>-->
<!--                A "pair" is a dictionary with two fields:-->
<!--                <br>-->
<!--                "input": the input "grid" for the pair.-->
<!--                <br>-->
<!--                "output": the output "grid" for the pair.-->
<!--            </div>-->
            <h4 class="w3-left-align" id="Bib"><b>Bibtex</b></h4>
            
            If this <a href="https://github.com/PRIS-CV/Fine-Grained-or-Not" target="__blank">work</a> is useful for you, please cite it:
            <div class="w3-code">
                @article{chang2021making,<br>
                &nbsp;&nbsp;&nbsp;&nbsp;title={Making a Bird AI Expert Work for You and Me},<br>
                &nbsp;&nbsp;&nbsp;&nbsp;author={Chang, Dongliang and Pang, Kaiyue and Du, Ruoyi and Ma, Zhanyu and Song, Yi-Zhe and Guo, Jun},<br>
                &nbsp;&nbsp;&nbsp;&nbsp;journal={arXiv preprint arXiv:2112.02747},<br>
                &nbsp;&nbsp;&nbsp;&nbsp;year={2021}<br>
                }
            </div>

<!-- ---------------------------------------- -->





<!-- -------------------------------------------------- -->






        </div>

        <hr/>  
        <div class="w3-content w3-center w3-opacity" style="max-width:850px"> <p style="font-size: xx-small;color: grey;">Proudly created by Dongliang Chang @ BUPT <br> 2021.6 </p> </div>

    </div>

</body>

</html>
